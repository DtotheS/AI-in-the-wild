{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Introduction\n",
    "* The notebook gives an introduction to NLP.\n",
    "* The notebook also introduce us to some preprocessing techniques required for text data\n",
    "* We will be working on the famous fake news dataset."
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Importing dataset",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "df = pd.read_csv('../input/fake-news/train.csv')\ntest = pd.read_csv('../input/fake-news/test.csv')",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "df.head()",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Data preprocessing and cleaning",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#filling NULL values with empty string\ndf=df.fillna('')\ntest=test.fillna('')",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# We will be only using title and author name for prediction\n# Creating new coolumn total concatenating title and author\ndf['total'] = df['title']+' '+df['author']\ntest['total']=test['title']+' '+test['author']",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "X = df.drop('label',axis=1)\ny=df['label']\nprint(X.shape)\nprint(y.shape)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#Choosing vocabulary size to be 5000 and copying data to msg for further cleaning\nvoc_size = 5000\nmsg = X.copy()\nmsg_test = test.copy()",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#Downloading stopwords \n#Stopwords are the words in any language which does not add much meaning to a sentence.\n#They can safely be ignored without sacrificing the meaning of the sentence.\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#We will be using Stemming here\n#Stemming map words to their root forms\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#Applying stemming and some preprocessing\nfor i in range(len(msg)):\n  review = re.sub('[^a-zA-Z]',' ',msg['total'][i])\n  review = review.lower()\n  review = review.split()\n  review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n  review = ' '.join(review)\n  corpus.append(review)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#Applying stemming and some preprocessing for test data\ncorpus_test = []\nfor i in range(len(msg_test)):\n  review = re.sub('[^a-zA-Z]',' ',msg_test['total'][i])\n  review = review.lower()\n  review = review.split()\n  review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n  review = ' '.join(review)\n  corpus_test.append(review)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Converting to one hot representation\nonehot_rep = [one_hot(words,voc_size)for words in corpus]\nonehot_rep_test = [one_hot(words,voc_size)for words in corpus_test]",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#Padding Sentences to make them of same size\nembedded_docs = pad_sequences(onehot_rep,padding='pre',maxlen=25)\nembedded_docs_test = pad_sequences(onehot_rep_test,padding='pre',maxlen=25)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Creating and training model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "#We have used embedding layers with LSTM\nmodel = Sequential()\nmodel.add(Embedding(voc_size,40,input_length=25))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#Converting into numpy array\nX_final = np.array(embedded_docs)\ny_final = np.array(y)\ntest_final = np.array(embedded_docs_test)\nX_final.shape,y_final.shape,test_final.shape",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "#training model\nmodel.fit(X_final,y_final,epochs=20,batch_size=64)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Creating Submission file ",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "y_pred = model.predict_classes(test_final)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "final_sub = pd.DataFrame()\nfinal_sub['id']=test['id']\nfinal_sub['label'] = y_pred\nfinal_sub.to_csv('final_sub.csv',index=False)",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "final_sub.head()",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Conclusion\n* LSTM with embedding layer works great\n* The model gives more than 99% accuracy on test data.\n* Furthermore we can also try vectoriztion or bi-directional LSTM.\n\n![](https://st3.depositphotos.com/1998651/13850/v/600/depositphotos_138506364-stock-illustration-cup-of-coffee-with-have.jpg)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  }
 ]
}